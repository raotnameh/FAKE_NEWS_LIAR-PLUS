{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import plot as plt\n",
    "\n",
    "\n",
    "torch.manual_seed(1)\n",
    "import cleaningtool as ct\n",
    "from helpers import *\n",
    "\n",
    "from model import *\n",
    "from data import *\n",
    "import sys\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning = True\n",
    "## Define paths\n",
    "DATA_FOLDER = './data_2/'\n",
    "TRAIN_PATH = DATA_FOLDER + 'train.tsv'\n",
    "TEST_PATH = DATA_FOLDER + 'test.tsv'\n",
    "VALID_PATH = DATA_FOLDER + 'valid.tsv'\n",
    "\n",
    "train_data = load_data_(TRAIN_PATH)\n",
    "test_data = load_data_(TEST_PATH)\n",
    "valid_data = load_data_(VALID_PATH)\n",
    "\n",
    "train_data = train_data[[\"statement\",\"justification\", \"label\"]]\n",
    "test_data = test_data[[\"statement\",\"justification\", \"label\"]]\n",
    "valid_data = valid_data[[\"statement\",\"justification\", \"label\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing NAN with an empthy string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.fillna('',inplace=True)\n",
    "test_data.fillna('',inplace=True)\n",
    "valid_data.fillna('',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging statement and justification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['input'] = train_data['statement'] + ' ' + train_data['justification'] \n",
    "test_data['input'] = test_data['statement'] + ' ' + test_data['justification']\n",
    "valid_data['input'] = valid_data['statement'] + ' ' + valid_data['justification'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[[\"input\", \"label\"]]\n",
    "test_data = test_data[[\"input\", \"label\"]]\n",
    "valid_data = valid_data[[\"input\", \"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.rename(columns={'input': 'statement'}, inplace=True)\n",
    "test_data.rename(columns={'input': 'statement'}, inplace=True)\n",
    "valid_data.rename(columns={'input': 'statement'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.concat([train_data, test_data, valid_data], axis=0, sort=False).reset_index()\n",
    "\n",
    "if cleaning == True:\n",
    "    print(\"before :-\",df_raw[\"statement\"][0])\n",
    "    train_data = clean_data(train_data,\"statement\")\n",
    "    test_data = clean_data(test_data,\"statement\")\n",
    "    valid_data = clean_data(valid_data,\"statement\")\n",
    "    df_raw = clean_data(df_raw,'statement')\n",
    "    print()\n",
    "    print(\"after :-\", df_raw[\"statement\"][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sentence to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_x, df_raw_y = sent_words(df_raw), sent_words(df_raw,label=True)\n",
    "x_train, y_train, x_val, y_val = sent_words(train_data), sent_words(train_data,label=True), sent_words(valid_data), sent_words(valid_data,label=True)\n",
    "x_test, y_test = sent_words(test_data), sent_words(test_data,label=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging x and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(x_train).reshape(len(x_train),1)\n",
    "y_train = np.array(y_train).reshape(len(x_train),1)\n",
    "x_test = np.array(x_test).reshape(len(x_test),1)\n",
    "y_test = np.array(y_test).reshape(len(x_test),1)\n",
    "x_val = np.array(x_val).reshape(len(x_val),1)\n",
    "y_val = np.array(y_val).reshape(len(x_val),1)\n",
    "\n",
    "train = np.concatenate((x_train,y_train),axis = 1)\n",
    "val = np.concatenate((x_val,y_val),axis = 1)\n",
    "test = np.concatenate((x_test,y_test),axis = 1)\n",
    "data_ = [train,val,test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating word_to_ix and label_to_ix dict and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = word_to_ix_(df_raw_x)\n",
    "label_to_ix = label_to_ix_(df_raw_y)\n",
    "\n",
    "ix_to_word = OrderedDict((v,k) for k,v in word_to_ix.items())\n",
    "ix_to_label = OrderedDict((v,k) for k,v in label_to_ix.items())\n",
    "VOCAB_SIZE = len( word_to_ix )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breaking them into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ = into_token(train,word_to_ix)\n",
    "test_ = into_token(test,word_to_ix)\n",
    "val_ = into_token(val,word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data_ = np.concatenate((train_,val_),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# importing embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('embed.p', 'rb') as fp:\n",
    "    embed = OrderedDict(pickle.load(fp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 300\n",
    "matrix_len = len(word_to_ix.keys())\n",
    "weights_matrix = np.zeros((matrix_len, emb_dim))\n",
    "words_found = 0\n",
    "\n",
    "for i, word in enumerate(word_to_ix.keys()):\n",
    "    try: \n",
    "        weights_matrix[i] = embed[word]\n",
    "        words_found += 1\n",
    "    except KeyError:\n",
    "        weights_matrix[i] = np.random.normal(scale=0.6, size=(emb_dim, ))\n",
    "print(\"words found :-\",words_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_matrix = torch.tensor(weights_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidirectional_ = True\n",
    "directions_ = 1\n",
    "if bidirectional_ == True:\n",
    "    directions_ = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# change it's value as per classification task requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_classes = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = create_emb_layer(weights_matrix, non_trainable=False)\n",
    "print(\"embed layer is trainable or no :----\",embedding[0].weight.requires_grad)\n",
    "model = EncoderRNN(embedding,hidden_size=512,num_layers=1,directions=directions_,bidirectonal=bidirectional_,out=out_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "#training\n",
    "update_weights = 512\n",
    "loss_t = []\n",
    "acc = 0\n",
    "epoch_ = 15\n",
    "for epoch in range(epoch_):\n",
    "    running_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    print(\"epoch number :\",epoch+1)\n",
    "    for i,(x,y) in enumerate(train_):\n",
    "        model.train()\n",
    "        \n",
    "        h = model.init_hidden(1).to(device)\n",
    "        y = torch.LongTensor(y).to(device)\n",
    "        inp = torch.tensor(x.T,dtype = torch.long).to(device)\n",
    "     \n",
    "        try:\n",
    "            out = model(inp,h)\n",
    "\n",
    "            loss = loss_function(out,y)\n",
    "            loss.backward()\n",
    "            loss_t.append(running_loss)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        except: pass\n",
    "\n",
    "        if i % update_weights == update_weights - 1:    # update weights as defined\t\n",
    "            \n",
    "            print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss/update_weights ))\n",
    "            running_loss = 0\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    torch.save(model.state_dict(),\"weights/embed_just\"+str(epoch)+\".pth\")\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        num = 0\n",
    "        length = 0\n",
    "        for i,(x,y) in enumerate(val_):\n",
    "            h = model.init_hidden(1).to(device)\n",
    "            y = torch.LongTensor(y).to(device)\n",
    "            inp = torch.tensor(x.T,dtype = torch.long).to(device)\n",
    "            try:\n",
    "                out = model(inp,h)\n",
    "                out,pred = torch.max(out,1)\n",
    "                if y == pred.item():\n",
    "                    num = num+1\n",
    "                length = length + 1\n",
    "            except: pass\n",
    "        accuracy = (num/length)*100\n",
    "            \n",
    "        print(\"accuray while evaluating is :\",accuracy ,\"%.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "for j in range(epoch_):\n",
    "    model.load_state_dict(torch.load(\"weights/embed_just\"+str(j)+\".pth\"), strict = True)\n",
    "    with torch.no_grad():\n",
    "            model.eval()\n",
    "            num = 0\n",
    "            length = 0\n",
    "            for i,(x,y) in enumerate(test_):\n",
    "                h = model.init_hidden(1).to(device)\n",
    "                y = torch.LongTensor(y).to(device)\n",
    "                inp = torch.tensor(x.T,dtype = torch.long).to(device)\n",
    "                try:\n",
    "                    out = model(inp,h)\n",
    "                    out,pred = torch.max(out,1)\n",
    "                    if y == pred.item():\n",
    "                        num = num+1\n",
    "                    length = length + 1\n",
    "                except: pass\n",
    "            accuracy = (num/length)*100\n",
    "            print(\"accuray while evaluating at\"+str(j)+\" is :\",accuracy,\"%.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
